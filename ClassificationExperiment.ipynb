{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss 26.050, test_loss 26.046\n",
      "epoch 2, train_loss 12.966, test_loss 12.964\n",
      "epoch 3, train_loss 7.337, test_loss 7.334\n",
      "epoch 4, train_loss 4.783, test_loss 4.781\n",
      "epoch 5, train_loss 3.487, test_loss 3.485\n",
      "epoch 6, train_loss 2.944, test_loss 2.942\n",
      "epoch 7, train_loss 2.644, test_loss 2.641\n",
      "epoch 8, train_loss 2.491, test_loss 2.487\n",
      "epoch 9, train_loss 2.419, test_loss 2.416\n",
      "epoch 10, train_loss 2.330, test_loss 2.326\n",
      "epoch 11, train_loss 2.335, test_loss 2.332\n",
      "epoch 12, train_loss 2.401, test_loss 2.397\n",
      "epoch 13, train_loss 2.285, test_loss 2.282\n",
      "epoch 14, train_loss 2.325, test_loss 2.321\n",
      "epoch 15, train_loss 2.357, test_loss 2.353\n",
      "epoch 16, train_loss 2.359, test_loss 2.356\n",
      "epoch 17, train_loss 2.328, test_loss 2.325\n",
      "epoch 18, train_loss 2.321, test_loss 2.317\n",
      "epoch 19, train_loss 2.333, test_loss 2.330\n",
      "epoch 20, train_loss 2.316, test_loss 2.313\n",
      "epoch 21, train_loss 2.313, test_loss 2.310\n",
      "epoch 22, train_loss 2.311, test_loss 2.308\n",
      "epoch 23, train_loss 2.317, test_loss 2.314\n",
      "epoch 24, train_loss 2.356, test_loss 2.352\n",
      "epoch 25, train_loss 2.356, test_loss 2.352\n",
      "epoch 26, train_loss 2.337, test_loss 2.333\n",
      "epoch 27, train_loss 2.333, test_loss 2.330\n",
      "epoch 28, train_loss 2.348, test_loss 2.345\n",
      "epoch 29, train_loss 2.373, test_loss 2.369\n",
      "epoch 30, train_loss 2.300, test_loss 2.297\n",
      "epoch 31, train_loss 2.348, test_loss 2.345\n",
      "epoch 32, train_loss 2.335, test_loss 2.332\n",
      "epoch 33, train_loss 2.311, test_loss 2.308\n",
      "epoch 34, train_loss 2.349, test_loss 2.346\n",
      "Loss on the test set stops decreasing for 20 times,triggered early stop\n",
      "best epoch 13, time 24.45s, best train_loss 2.285, for test_loss 2.282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training accuracy: 84.251098% of NAG\n",
      "Test accuracy: 84.626251% of NAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss 47.684, test_loss 47.707\n",
      "epoch 2, train_loss 33.495, test_loss 33.504\n",
      "epoch 3, train_loss 23.133, test_loss 23.143\n",
      "epoch 4, train_loss 16.101, test_loss 16.108\n",
      "epoch 5, train_loss 11.434, test_loss 11.437\n",
      "epoch 6, train_loss 8.495, test_loss 8.496\n",
      "epoch 7, train_loss 6.748, test_loss 6.747\n",
      "epoch 8, train_loss 5.733, test_loss 5.732\n",
      "epoch 9, train_loss 5.012, test_loss 5.010\n",
      "epoch 10, train_loss 4.515, test_loss 4.512\n",
      "epoch 11, train_loss 4.105, test_loss 4.102\n",
      "epoch 12, train_loss 3.771, test_loss 3.768\n",
      "epoch 13, train_loss 3.508, test_loss 3.506\n",
      "epoch 14, train_loss 3.295, test_loss 3.292\n",
      "epoch 15, train_loss 3.112, test_loss 3.109\n",
      "epoch 16, train_loss 2.966, test_loss 2.963\n",
      "epoch 17, train_loss 2.835, test_loss 2.832\n",
      "epoch 18, train_loss 2.752, test_loss 2.748\n",
      "epoch 19, train_loss 2.679, test_loss 2.675\n",
      "epoch 20, train_loss 2.619, test_loss 2.615\n",
      "epoch 21, train_loss 2.555, test_loss 2.551\n",
      "epoch 22, train_loss 2.512, test_loss 2.509\n",
      "epoch 23, train_loss 2.445, test_loss 2.442\n",
      "epoch 24, train_loss 2.433, test_loss 2.430\n",
      "epoch 25, train_loss 2.399, test_loss 2.396\n",
      "epoch 26, train_loss 2.373, test_loss 2.370\n",
      "epoch 27, train_loss 2.354, test_loss 2.351\n",
      "epoch 28, train_loss 2.342, test_loss 2.339\n",
      "epoch 29, train_loss 2.312, test_loss 2.309\n",
      "epoch 30, train_loss 2.310, test_loss 2.307\n",
      "epoch 31, train_loss 2.286, test_loss 2.283\n",
      "epoch 32, train_loss 2.296, test_loss 2.293\n",
      "epoch 33, train_loss 2.297, test_loss 2.294\n",
      "epoch 34, train_loss 2.290, test_loss 2.286\n",
      "epoch 35, train_loss 2.292, test_loss 2.288\n",
      "epoch 36, train_loss 2.275, test_loss 2.271\n",
      "epoch 37, train_loss 2.275, test_loss 2.272\n",
      "epoch 38, train_loss 2.279, test_loss 2.276\n",
      "epoch 39, train_loss 2.286, test_loss 2.282\n",
      "epoch 40, train_loss 2.283, test_loss 2.280\n",
      "epoch 41, train_loss 2.271, test_loss 2.267\n",
      "epoch 42, train_loss 2.275, test_loss 2.272\n",
      "epoch 43, train_loss 2.282, test_loss 2.279\n",
      "epoch 44, train_loss 2.270, test_loss 2.267\n",
      "epoch 45, train_loss 2.277, test_loss 2.274\n",
      "epoch 46, train_loss 2.288, test_loss 2.284\n",
      "epoch 47, train_loss 2.286, test_loss 2.283\n",
      "epoch 48, train_loss 2.282, test_loss 2.278\n",
      "epoch 49, train_loss 2.269, test_loss 2.266\n",
      "epoch 50, train_loss 2.277, test_loss 2.274\n",
      "epoch 51, train_loss 2.277, test_loss 2.274\n",
      "epoch 52, train_loss 2.274, test_loss 2.271\n",
      "epoch 53, train_loss 2.283, test_loss 2.280\n",
      "epoch 54, train_loss 2.287, test_loss 2.283\n",
      "epoch 55, train_loss 2.302, test_loss 2.298\n",
      "epoch 56, train_loss 2.304, test_loss 2.300\n",
      "epoch 57, train_loss 2.289, test_loss 2.285\n",
      "epoch 58, train_loss 2.298, test_loss 2.295\n",
      "epoch 59, train_loss 2.274, test_loss 2.270\n",
      "epoch 60, train_loss 2.302, test_loss 2.299\n",
      "epoch 61, train_loss 2.294, test_loss 2.291\n",
      "epoch 62, train_loss 2.281, test_loss 2.278\n",
      "epoch 63, train_loss 2.270, test_loss 2.267\n",
      "epoch 64, train_loss 2.273, test_loss 2.270\n",
      "epoch 65, train_loss 2.269, test_loss 2.266\n",
      "epoch 66, train_loss 2.284, test_loss 2.281\n",
      "epoch 67, train_loss 2.303, test_loss 2.299\n",
      "epoch 68, train_loss 2.293, test_loss 2.290\n",
      "epoch 69, train_loss 2.302, test_loss 2.299\n",
      "epoch 70, train_loss 2.299, test_loss 2.295\n",
      "Loss on the test set stops decreasing for 20 times,triggered early stop\n",
      "best epoch 49, time 48.71s, best train_loss 2.269, for test_loss 2.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training accuracy: 84.269525% of RMSprop\n",
      "Test accuracy: 84.687673% of RMSprop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss 64.745, test_loss 64.738\n",
      "epoch 2, train_loss 59.317, test_loss 59.311\n",
      "epoch 3, train_loss 54.466, test_loss 54.460\n",
      "epoch 4, train_loss 50.091, test_loss 50.087\n",
      "epoch 5, train_loss 46.161, test_loss 46.158\n",
      "epoch 6, train_loss 42.599, test_loss 42.597\n",
      "epoch 7, train_loss 39.383, test_loss 39.381\n",
      "epoch 8, train_loss 36.459, test_loss 36.457\n",
      "epoch 9, train_loss 33.810, test_loss 33.808\n",
      "epoch 10, train_loss 31.379, test_loss 31.377\n",
      "epoch 11, train_loss 29.178, test_loss 29.176\n",
      "epoch 12, train_loss 27.174, test_loss 27.173\n",
      "epoch 13, train_loss 25.339, test_loss 25.338\n",
      "epoch 14, train_loss 23.664, test_loss 23.663\n",
      "epoch 15, train_loss 22.122, test_loss 22.120\n",
      "epoch 16, train_loss 20.702, test_loss 20.700\n",
      "epoch 17, train_loss 19.395, test_loss 19.393\n",
      "epoch 18, train_loss 18.190, test_loss 18.188\n",
      "epoch 19, train_loss 17.068, test_loss 17.066\n",
      "epoch 20, train_loss 16.030, test_loss 16.028\n",
      "epoch 21, train_loss 15.073, test_loss 15.071\n",
      "epoch 22, train_loss 14.192, test_loss 14.190\n",
      "epoch 23, train_loss 13.363, test_loss 13.361\n",
      "epoch 24, train_loss 12.598, test_loss 12.596\n",
      "epoch 25, train_loss 11.887, test_loss 11.885\n",
      "epoch 26, train_loss 11.228, test_loss 11.226\n",
      "epoch 27, train_loss 10.615, test_loss 10.612\n",
      "epoch 28, train_loss 10.038, test_loss 10.036\n",
      "epoch 29, train_loss 9.506, test_loss 9.504\n",
      "epoch 30, train_loss 9.015, test_loss 9.012\n",
      "epoch 31, train_loss 8.562, test_loss 8.559\n",
      "epoch 32, train_loss 8.123, test_loss 8.120\n",
      "epoch 33, train_loss 7.722, test_loss 7.720\n",
      "epoch 34, train_loss 7.354, test_loss 7.351\n",
      "epoch 35, train_loss 7.016, test_loss 7.013\n",
      "epoch 36, train_loss 6.695, test_loss 6.693\n",
      "epoch 37, train_loss 6.396, test_loss 6.393\n",
      "epoch 38, train_loss 6.120, test_loss 6.117\n",
      "epoch 39, train_loss 5.858, test_loss 5.855\n",
      "epoch 40, train_loss 5.618, test_loss 5.615\n",
      "epoch 41, train_loss 5.399, test_loss 5.396\n",
      "epoch 42, train_loss 5.190, test_loss 5.187\n",
      "epoch 43, train_loss 4.992, test_loss 4.989\n",
      "epoch 44, train_loss 4.813, test_loss 4.810\n",
      "epoch 45, train_loss 4.644, test_loss 4.641\n",
      "epoch 46, train_loss 4.489, test_loss 4.486\n",
      "epoch 47, train_loss 4.342, test_loss 4.339\n",
      "epoch 48, train_loss 4.207, test_loss 4.204\n",
      "epoch 49, train_loss 4.085, test_loss 4.081\n",
      "epoch 50, train_loss 3.967, test_loss 3.963\n",
      "epoch 51, train_loss 3.848, test_loss 3.844\n",
      "epoch 52, train_loss 3.747, test_loss 3.744\n",
      "epoch 53, train_loss 3.655, test_loss 3.652\n",
      "epoch 54, train_loss 3.567, test_loss 3.563\n",
      "epoch 55, train_loss 3.486, test_loss 3.483\n",
      "epoch 56, train_loss 3.413, test_loss 3.410\n",
      "epoch 57, train_loss 3.337, test_loss 3.334\n",
      "epoch 58, train_loss 3.273, test_loss 3.270\n",
      "epoch 59, train_loss 3.213, test_loss 3.210\n",
      "epoch 60, train_loss 3.143, test_loss 3.140\n",
      "epoch 61, train_loss 3.093, test_loss 3.090\n",
      "epoch 62, train_loss 3.042, test_loss 3.039\n",
      "epoch 63, train_loss 2.993, test_loss 2.990\n",
      "epoch 64, train_loss 2.946, test_loss 2.943\n",
      "epoch 65, train_loss 2.905, test_loss 2.901\n",
      "epoch 66, train_loss 2.867, test_loss 2.864\n",
      "epoch 67, train_loss 2.835, test_loss 2.832\n",
      "epoch 68, train_loss 2.796, test_loss 2.793\n",
      "epoch 69, train_loss 2.767, test_loss 2.764\n",
      "epoch 70, train_loss 2.734, test_loss 2.730\n",
      "epoch 71, train_loss 2.708, test_loss 2.704\n",
      "epoch 72, train_loss 2.680, test_loss 2.676\n",
      "epoch 73, train_loss 2.658, test_loss 2.654\n",
      "epoch 74, train_loss 2.634, test_loss 2.631\n",
      "epoch 75, train_loss 2.612, test_loss 2.609\n",
      "epoch 76, train_loss 2.596, test_loss 2.593\n",
      "epoch 77, train_loss 2.579, test_loss 2.576\n",
      "epoch 78, train_loss 2.563, test_loss 2.560\n",
      "epoch 79, train_loss 2.546, test_loss 2.542\n",
      "epoch 80, train_loss 2.530, test_loss 2.527\n",
      "epoch 81, train_loss 2.514, test_loss 2.510\n",
      "epoch 82, train_loss 2.503, test_loss 2.499\n",
      "epoch 83, train_loss 2.490, test_loss 2.486\n",
      "epoch 84, train_loss 2.481, test_loss 2.478\n",
      "epoch 85, train_loss 2.473, test_loss 2.470\n",
      "epoch 86, train_loss 2.461, test_loss 2.457\n",
      "epoch 87, train_loss 2.455, test_loss 2.452\n",
      "epoch 88, train_loss 2.443, test_loss 2.439\n",
      "epoch 89, train_loss 2.433, test_loss 2.430\n",
      "epoch 90, train_loss 2.425, test_loss 2.422\n",
      "epoch 91, train_loss 2.416, test_loss 2.412\n",
      "epoch 92, train_loss 2.410, test_loss 2.407\n",
      "epoch 93, train_loss 2.405, test_loss 2.401\n",
      "epoch 94, train_loss 2.400, test_loss 2.396\n",
      "epoch 95, train_loss 2.392, test_loss 2.389\n",
      "epoch 96, train_loss 2.381, test_loss 2.378\n",
      "epoch 97, train_loss 2.379, test_loss 2.376\n",
      "epoch 98, train_loss 2.379, test_loss 2.376\n",
      "epoch 99, train_loss 2.375, test_loss 2.371\n",
      "epoch 100, train_loss 2.369, test_loss 2.365\n",
      "epoch 101, train_loss 2.365, test_loss 2.362\n",
      "epoch 102, train_loss 2.357, test_loss 2.354\n",
      "epoch 103, train_loss 2.356, test_loss 2.352\n",
      "epoch 104, train_loss 2.351, test_loss 2.348\n",
      "epoch 105, train_loss 2.356, test_loss 2.353\n",
      "epoch 106, train_loss 2.353, test_loss 2.350\n",
      "epoch 107, train_loss 2.350, test_loss 2.347\n",
      "epoch 108, train_loss 2.346, test_loss 2.342\n",
      "epoch 109, train_loss 2.343, test_loss 2.339\n",
      "epoch 110, train_loss 2.345, test_loss 2.342\n",
      "epoch 111, train_loss 2.345, test_loss 2.342\n",
      "epoch 112, train_loss 2.346, test_loss 2.343\n",
      "epoch 113, train_loss 2.347, test_loss 2.343\n",
      "epoch 114, train_loss 2.344, test_loss 2.341\n",
      "epoch 115, train_loss 2.339, test_loss 2.336\n",
      "epoch 116, train_loss 2.337, test_loss 2.334\n",
      "epoch 117, train_loss 2.340, test_loss 2.337\n",
      "epoch 118, train_loss 2.338, test_loss 2.334\n",
      "epoch 119, train_loss 2.338, test_loss 2.335\n",
      "epoch 120, train_loss 2.334, test_loss 2.330\n",
      "epoch 121, train_loss 2.333, test_loss 2.330\n",
      "epoch 122, train_loss 2.333, test_loss 2.330\n",
      "epoch 123, train_loss 2.335, test_loss 2.332\n",
      "epoch 124, train_loss 2.335, test_loss 2.332\n",
      "epoch 125, train_loss 2.334, test_loss 2.330\n",
      "epoch 126, train_loss 2.332, test_loss 2.329\n",
      "epoch 127, train_loss 2.326, test_loss 2.322\n",
      "epoch 128, train_loss 2.327, test_loss 2.324\n",
      "epoch 129, train_loss 2.324, test_loss 2.321\n",
      "epoch 130, train_loss 2.327, test_loss 2.324\n",
      "epoch 131, train_loss 2.328, test_loss 2.324\n",
      "epoch 132, train_loss 2.328, test_loss 2.324\n",
      "epoch 133, train_loss 2.331, test_loss 2.327\n",
      "epoch 134, train_loss 2.333, test_loss 2.329\n",
      "epoch 135, train_loss 2.335, test_loss 2.331\n",
      "epoch 136, train_loss 2.331, test_loss 2.327\n",
      "epoch 137, train_loss 2.323, test_loss 2.319\n",
      "epoch 138, train_loss 2.322, test_loss 2.319\n",
      "epoch 139, train_loss 2.323, test_loss 2.320\n",
      "epoch 140, train_loss 2.329, test_loss 2.325\n",
      "epoch 141, train_loss 2.325, test_loss 2.322\n",
      "epoch 142, train_loss 2.327, test_loss 2.324\n",
      "epoch 143, train_loss 2.326, test_loss 2.323\n",
      "epoch 144, train_loss 2.325, test_loss 2.322\n",
      "epoch 145, train_loss 2.325, test_loss 2.321\n",
      "epoch 146, train_loss 2.325, test_loss 2.321\n",
      "epoch 147, train_loss 2.321, test_loss 2.318\n",
      "epoch 148, train_loss 2.322, test_loss 2.318\n",
      "epoch 149, train_loss 2.322, test_loss 2.318\n",
      "epoch 150, train_loss 2.325, test_loss 2.321\n",
      "epoch 151, train_loss 2.323, test_loss 2.319\n",
      "epoch 152, train_loss 2.319, test_loss 2.316\n",
      "epoch 153, train_loss 2.320, test_loss 2.317\n",
      "epoch 154, train_loss 2.320, test_loss 2.317\n",
      "epoch 155, train_loss 2.327, test_loss 2.324\n",
      "epoch 156, train_loss 2.326, test_loss 2.323\n",
      "epoch 157, train_loss 2.324, test_loss 2.320\n",
      "epoch 158, train_loss 2.322, test_loss 2.319\n",
      "epoch 159, train_loss 2.324, test_loss 2.321\n",
      "epoch 160, train_loss 2.324, test_loss 2.320\n",
      "epoch 161, train_loss 2.321, test_loss 2.318\n",
      "epoch 162, train_loss 2.322, test_loss 2.318\n",
      "epoch 163, train_loss 2.319, test_loss 2.316\n",
      "epoch 164, train_loss 2.322, test_loss 2.319\n",
      "epoch 165, train_loss 2.325, test_loss 2.321\n",
      "epoch 166, train_loss 2.324, test_loss 2.321\n",
      "epoch 167, train_loss 2.325, test_loss 2.322\n",
      "epoch 168, train_loss 2.321, test_loss 2.318\n",
      "epoch 169, train_loss 2.323, test_loss 2.320\n",
      "epoch 170, train_loss 2.327, test_loss 2.323\n",
      "epoch 171, train_loss 2.323, test_loss 2.320\n",
      "epoch 172, train_loss 2.325, test_loss 2.322\n",
      "epoch 173, train_loss 2.329, test_loss 2.325\n",
      "Loss on the test set stops decreasing for 20 times,triggered early stop\n",
      "best epoch 152, time 218.93s, best train_loss 2.319, for test_loss 2.316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training accuracy: 84.260311% of Adadelta\n",
      "Test accuracy: 84.632394% of Adadelta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss 53.601, test_loss 53.601\n",
      "epoch 2, train_loss 41.970, test_loss 41.967\n",
      "epoch 3, train_loss 33.718, test_loss 33.714\n",
      "epoch 4, train_loss 27.567, test_loss 27.561\n",
      "epoch 5, train_loss 22.839, test_loss 22.832\n",
      "epoch 6, train_loss 19.155, test_loss 19.148\n",
      "epoch 7, train_loss 16.190, test_loss 16.183\n",
      "epoch 8, train_loss 13.765, test_loss 13.759\n",
      "epoch 9, train_loss 11.769, test_loss 11.763\n",
      "epoch 10, train_loss 10.065, test_loss 10.059\n",
      "epoch 11, train_loss 8.628, test_loss 8.622\n",
      "epoch 12, train_loss 7.416, test_loss 7.410\n",
      "epoch 13, train_loss 6.440, test_loss 6.434\n",
      "epoch 14, train_loss 5.607, test_loss 5.602\n",
      "epoch 15, train_loss 4.918, test_loss 4.912\n",
      "epoch 16, train_loss 4.379, test_loss 4.374\n",
      "epoch 17, train_loss 3.942, test_loss 3.936\n",
      "epoch 18, train_loss 3.604, test_loss 3.599\n",
      "epoch 19, train_loss 3.326, test_loss 3.321\n",
      "epoch 20, train_loss 3.123, test_loss 3.118\n",
      "epoch 21, train_loss 2.949, test_loss 2.945\n",
      "epoch 22, train_loss 2.815, test_loss 2.810\n",
      "epoch 23, train_loss 2.702, test_loss 2.698\n",
      "epoch 24, train_loss 2.620, test_loss 2.616\n",
      "epoch 25, train_loss 2.559, test_loss 2.555\n",
      "epoch 26, train_loss 2.508, test_loss 2.504\n",
      "epoch 27, train_loss 2.482, test_loss 2.479\n",
      "epoch 28, train_loss 2.453, test_loss 2.449\n",
      "epoch 29, train_loss 2.404, test_loss 2.400\n",
      "epoch 30, train_loss 2.404, test_loss 2.400\n",
      "epoch 31, train_loss 2.373, test_loss 2.370\n",
      "epoch 32, train_loss 2.370, test_loss 2.366\n",
      "epoch 33, train_loss 2.363, test_loss 2.359\n",
      "epoch 34, train_loss 2.345, test_loss 2.342\n",
      "epoch 35, train_loss 2.339, test_loss 2.335\n",
      "epoch 36, train_loss 2.346, test_loss 2.342\n",
      "epoch 37, train_loss 2.329, test_loss 2.326\n",
      "epoch 38, train_loss 2.331, test_loss 2.328\n",
      "epoch 39, train_loss 2.317, test_loss 2.314\n",
      "epoch 40, train_loss 2.321, test_loss 2.318\n",
      "epoch 41, train_loss 2.331, test_loss 2.327\n",
      "epoch 42, train_loss 2.317, test_loss 2.313\n",
      "epoch 43, train_loss 2.317, test_loss 2.313\n",
      "epoch 44, train_loss 2.312, test_loss 2.308\n",
      "epoch 45, train_loss 2.316, test_loss 2.312\n",
      "epoch 46, train_loss 2.319, test_loss 2.315\n",
      "epoch 47, train_loss 2.312, test_loss 2.309\n",
      "epoch 48, train_loss 2.315, test_loss 2.311\n",
      "epoch 49, train_loss 2.313, test_loss 2.309\n",
      "epoch 50, train_loss 2.309, test_loss 2.306\n",
      "epoch 51, train_loss 2.311, test_loss 2.308\n",
      "epoch 52, train_loss 2.308, test_loss 2.305\n",
      "epoch 53, train_loss 2.308, test_loss 2.304\n",
      "epoch 54, train_loss 2.299, test_loss 2.295\n",
      "epoch 55, train_loss 2.310, test_loss 2.307\n",
      "epoch 56, train_loss 2.309, test_loss 2.306\n",
      "epoch 57, train_loss 2.305, test_loss 2.301\n",
      "epoch 58, train_loss 2.315, test_loss 2.312\n",
      "epoch 59, train_loss 2.309, test_loss 2.305\n",
      "epoch 60, train_loss 2.311, test_loss 2.308\n",
      "epoch 61, train_loss 2.308, test_loss 2.305\n",
      "epoch 62, train_loss 2.317, test_loss 2.314\n",
      "epoch 63, train_loss 2.306, test_loss 2.303\n",
      "epoch 64, train_loss 2.313, test_loss 2.309\n",
      "epoch 65, train_loss 2.319, test_loss 2.315\n",
      "epoch 66, train_loss 2.310, test_loss 2.307\n",
      "epoch 67, train_loss 2.313, test_loss 2.309\n",
      "epoch 68, train_loss 2.325, test_loss 2.322\n",
      "epoch 69, train_loss 2.306, test_loss 2.303\n",
      "epoch 70, train_loss 2.314, test_loss 2.311\n",
      "epoch 71, train_loss 2.320, test_loss 2.316\n",
      "epoch 72, train_loss 2.318, test_loss 2.315\n",
      "epoch 73, train_loss 2.313, test_loss 2.310\n",
      "epoch 74, train_loss 2.320, test_loss 2.317\n",
      "epoch 75, train_loss 2.332, test_loss 2.329\n",
      "Loss on the test set stops decreasing for 20 times,triggered early stop\n",
      "best epoch 54, time 51.25s, best train_loss 2.299, for test_loss 2.295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training accuracy: 84.254169% of Adam\n",
      "Test accuracy: 84.650820% of Adam\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngTHGZLj/D0mWIEamBpiHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f24906df890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#encoding:utf-8\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import inspect\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "plt_color_array = ['blue', 'green', 'red', 'yellow']\n",
    "plt_dict = dict()\n",
    "opt_algo_set = [ 'NAG', 'RMSprop','Adadelta', 'Adam']\n",
    "\n",
    "def HingeLoss(y, y_true):\n",
    "\n",
    "    if not (y.shape[0] == y_true.shape[0]):\n",
    "        print 'Mismatching of input ndarray input shapes in function \"HingeLoss\", line:', cline()\n",
    "        print 'y.shape =', y.shape, 'y_true.shape =', y.shape\n",
    "        sys.exit()\n",
    "    #output=max(0,1-y*y_true)\n",
    "    output = 1.0 - (y * y_true)\n",
    "    output *= np.asarray((output > 0.0),dtype=float)\n",
    "\n",
    "    return output\n",
    "\n",
    "def Accuracy(y, y_true):\n",
    "\n",
    "    if not (y.shape[0] == y_true.shape[0]):\n",
    "        print 'Mismatching of input ndarray input shapes in function \"Accuracy\", line:', cline()\n",
    "        print 'y.shape =', y.shape, 'y_true.shape =', y.shape\n",
    "        sys.exit()\n",
    "\n",
    "    output = np.sum(y_true == y) / float(y.shape[0])\n",
    "\n",
    "    return output\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, model, x, y,opt_algo='SGD', batch_size=1, lambda_=1.0,\n",
    "                 learning_rate=0.0001, loss_type='HingeLoss',\n",
    "                 regularization='L2'):\n",
    "\n",
    "        self._loss_type = loss_type\n",
    "        self._regularization = regularization\n",
    "        self._lambda = lambda_\n",
    "        self._lrate = learning_rate\n",
    "        self._bsize = batch_size\n",
    "        self._model = model\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.used_samples = np.zeros(self.x.shape[0])\n",
    "        self.opt_algo=opt_algo\n",
    "        self.first_run = True\n",
    "\n",
    "        self.v = np.zeros(self._model.features_size + 1)\n",
    "        self.m = np.zeros(self._model.features_size + 1)\n",
    "        self.grad_expect = np.zeros(self._model.features_size + 1)\n",
    "        self.beta1_exp = 1.0\n",
    "        self.beta2_exp = 1.0\n",
    "        self.delta_expect = np.zeros(self._model.features_size + 1)\n",
    "\n",
    "\n",
    "    def get_batch(self): # start name of func with _\n",
    "\n",
    "        idxs = np.random.choice(np.where(self.used_samples == 0)[0], self._bsize)\n",
    "        batch_x = self.x[idxs]\n",
    "        batch_y = self.y[idxs]\n",
    "        self.used_samples[idxs] = 1.0\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def calc_loss(self, batch_x, batch_y, loss=None):\n",
    "\n",
    "        if not (batch_x.shape[0] == batch_y.shape[0]):\n",
    "            print 'Input arrays shapes mismatching in \\\n",
    "                    SGD.calc_loss(), line', cline()\n",
    "            sys.exit()\n",
    "        if loss is None:\n",
    "            loss = self._loss_type\n",
    "        output_loss = 0.0\n",
    "\n",
    "        # HingeLoss\n",
    "        if loss == 'HingeLoss':\n",
    "            batch_y_predicted = self._model.predict(batch_x, binar=False)\n",
    "            output_loss = np.mean(HingeLoss(batch_y_predicted, batch_y))\n",
    "\n",
    "        # Accuracy\n",
    "        elif loss == 'Accuracy':\n",
    "            batch_y_predicted = self._model.predict(batch_x)\n",
    "            output_loss = Accuracy(batch_y_predicted, batch_y)\n",
    "        else:\n",
    "            print 'Unknown loss type in SGD.calc_loss, line', cline()\n",
    "            sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "        if self._regularization == None:\n",
    "            return output_loss\n",
    "        elif self._regularization == 'L2':\n",
    "            if not self._loss_type == 'Accuracy':\n",
    "                output_loss += 0.5 * self._lambda * np.sum(self._model.wb[:-1]**2)\n",
    "            return output_loss\n",
    "        else:\n",
    "            print 'Unknown regularization type in SGD.calc_loss, line', cline()\n",
    "            sys.exit()\n",
    "\n",
    "    def grad(self, batch_x, batch_y):\n",
    "        if not (batch_x.shape[0] == batch_y.shape[0]):\n",
    "            print 'Input arrays shapes mismatching in \\\n",
    "                    SGD.grad(), line', cline()\n",
    "            sys.exit()\n",
    "\n",
    "        grad_matr = np.zeros([self._bsize, self._model.features_size + 1])\n",
    "        output = np.zeros(self._model.features_size + 1)\n",
    "        if self._loss_type == 'HingeLoss':\n",
    "            batch_y_predicted = self._model.predict(batch_x, binar=False)\n",
    "            current_loss = HingeLoss(batch_y_predicted, batch_y)\n",
    "            for i in range(self._bsize):\n",
    "                if current_loss[i] == 0.0:\n",
    "                    continue\n",
    "                else:\n",
    "                    for j in range(self._model.features_size):\n",
    "                        grad_matr[i, j] = -batch_y[i] * batch_x[i, j]\n",
    "                    grad_matr[i,self._model.features_size] = - batch_y[i]\n",
    "\n",
    "            output = np.sum(grad_matr, axis=0)\n",
    "\n",
    "        if self._regularization == None:\n",
    "            return output\n",
    "        elif self._regularization == 'L2':\n",
    "            return output + self._lambda * self._model.wb\n",
    "\n",
    "    def step(self, batch_x, batch_y):\n",
    "        gamma = 0.9\n",
    "        epsilon = 1e-8\n",
    "        if self.opt_algo == 'RMSprop' or self.opt_algo == 'Adam':\n",
    "            self._lrate = 0.001\n",
    "        else:\n",
    "            self._lrate = 0.0001\n",
    "\n",
    "        # Adam params\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "\n",
    "        if self.opt_algo=='SGD':\n",
    "            self._model.wb -= self._lrate * self.grad(batch_x, batch_y)\n",
    "        elif self.opt_algo=='NAG':\n",
    "            self.v = gamma * self.v + self._lrate * self.grad(batch_x, batch_y)\n",
    "            self._model.wb = self._model.wb - self.v\n",
    "        elif self.opt_algo == 'Adadelta':\n",
    "            self.grad_expect = gamma * self.grad_expect + (1.0 - gamma) * np.square(self.grad(batch_x, batch_y))\n",
    "            if self.first_run == True:\n",
    "                delta = - self._lrate * self.grad(batch_x, batch_y)\n",
    "            else:\n",
    "                delta = - np.multiply(np.sqrt(self.delta_expect + epsilon) / np.sqrt(self.grad_expect + epsilon), self.grad(batch_x, batch_y))\n",
    "            self._model.wb = self._model.wb + delta\n",
    "            self.delta_expect = gamma * self.delta_expect + (1.0 - gamma) * np.square(delta)\n",
    "        elif self.opt_algo == 'RMSprop':\n",
    "            # RMSprop\n",
    "            grad = self.grad(batch_x, batch_y)\n",
    "            self.grad_expect = gamma * self.grad_expect + (1.0 - gamma) * np.square(grad)\n",
    "            self._model.wb = self._model.wb - self._lrate * grad / np.sqrt(self.grad_expect + epsilon)\n",
    "        elif self.opt_algo == 'Adam':\n",
    "            # Adam\n",
    "            grad = self.grad(batch_x, batch_y)\n",
    "            self.m = beta1 * self.m + (1.0 - beta1) * grad\n",
    "            self.v = beta2 * self.v + (1.0 - beta2) * np.square(grad)\n",
    "            self.beta1_exp *= beta1\n",
    "            self.beta2_exp *= beta2\n",
    "            self._model.wb = self._model.wb - self._lrate * (self.m / (1.0 - self.beta1_exp)) / (np.sqrt(self.v / (1.0 - self.beta2_exp)) + epsilon)\n",
    "\n",
    "        if self.first_run == True: first_run = False\n",
    "\n",
    "    def make_epoch(self):\n",
    "        while np.sum(self.used_samples) < self.used_samples.shape[0]:\n",
    "            batch_x, batch_y = self.get_batch()\n",
    "            self.step(batch_x, batch_y)\n",
    "\n",
    "        #back to initial statement\n",
    "        self.used_samples = np.zeros(self.x.shape[0])\n",
    "\n",
    "VERY_BIG_NUMBER = 70.0\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, loss='HingeLoss', optimizer='SGD',\n",
    "                 chronicle_loss_history=True, chronicle_model_history=False,earlystop=20):\n",
    "        #! make comments in correct form\n",
    "        # Here is defined model parameters as one array, where\n",
    "        # 'b' is represented as the last element in wb\n",
    "        self.istrained = False\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.wb = None\n",
    "\n",
    "        self.epoch_learned = 0\n",
    "\n",
    "        self.train_loss = VERY_BIG_NUMBER\n",
    "        self.test_loss = VERY_BIG_NUMBER\n",
    "        self.early_stop=earlystop\n",
    "\n",
    "\n",
    "        if chronicle_loss_history:\n",
    "            self.train_loss_history  = []\n",
    "        else:\n",
    "            self.train_loss_history  = None\n",
    "\n",
    "        if chronicle_model_history:\n",
    "            self.train_model_history = []\n",
    "        else:\n",
    "            self.train_model_history = None\n",
    "\n",
    "    #! fit doesn't use X_test, I need new func for evaluation\n",
    "    def fit(self, X_train, y_train, opt_algo,X_test=None, y_test=None, batch_size=15,\n",
    "            n_epoch=1, learning_rate=0.0001, verbose=True):\n",
    "\n",
    "        time_start = time.time()\n",
    "        if not self.optimizer == 'SGD':\n",
    "            print 'Unknown optimizer type! SVM.fit, line', cline()\n",
    "            sys.exit()\n",
    "\n",
    "        if not X_train.ndim == 2:\n",
    "            print 'X_train has bad shapes in SVM.fit, line', cline()\n",
    "            sys.exit()\n",
    "\n",
    "        if not (X_train.shape[0] == y_train.shape[0]):\n",
    "            print 'Input arrays shapes mismatching in SVM.fit, line', cline()\n",
    "            sys.exit()\n",
    "\n",
    "        if not self.istrained:\n",
    "            self.features_size = X_train.shape[1]\n",
    "            self.wb = np.random.randn(self.features_size + 1) #  采用随机初始化\n",
    "            # self.wb = np.zeros(self.features_size + 1)\n",
    "            if not (self.train_loss_history is None):\n",
    "                self.train_loss_history.append([self.train_loss, self.test_loss])\n",
    "            if not (self.train_model_history is None):\n",
    "                self.train_model_history.append(list(self.wb))\n",
    "\n",
    "        if self.optimizer == 'SGD':\n",
    "            if self.epoch_learned == 0:\n",
    "                solver = SGD(self, X_train, y_train, loss_type=self.loss,\n",
    "                             batch_size=batch_size, learning_rate=learning_rate,opt_algo=opt_algo)\n",
    "\n",
    "            min_loss = VERY_BIG_NUMBER\n",
    "            best_epoch=VERY_BIG_NUMBER\n",
    "            min_loss_train=VERY_BIG_NUMBER\n",
    "            without_updates = 0\n",
    "            while self.epoch_learned < n_epoch:\n",
    "                solver.make_epoch()\n",
    "                self.epoch_learned += 1\n",
    "                self.train_loss = solver.calc_loss(X_train, y_train)\n",
    "\n",
    "                if not (X_test is None):\n",
    "                    self.test_loss = solver.calc_loss(X_test, y_test)\n",
    "\n",
    "                if verbose:\n",
    "                    print 'epoch %d, train_loss %1.3lf, test_loss %1.3lf'%\\\n",
    "                            (self.epoch_learned, self.train_loss, self.test_loss)\n",
    "                self.istrained = True\n",
    "                if self.test_loss < min_loss:\n",
    "                    min_loss = self.test_loss\n",
    "                    min_loss_train=self.train_loss\n",
    "                    best_epoch=self.epoch_learned\n",
    "                    without_updates = 0\n",
    "                else:\n",
    "                    without_updates += 1\n",
    "                    if without_updates > self.early_stop:\n",
    "                        print 'Loss on the test set stops decreasing for',self.early_stop,'times,triggered early stop'\n",
    "                        break\n",
    "                if not (self.train_loss_history is None):\n",
    "                    self.train_loss_history.append([self.train_loss, self.test_loss])\n",
    "                if not (self.train_model_history is None):\n",
    "                    self.train_model_history.append(list(self.wb))\n",
    "\n",
    "        time_finish = time.time()\n",
    "        print 'best epoch %d, time %1.2lfs, best train_loss %1.3lf, for test_loss %1.3lf'%\\\n",
    "                (best_epoch, time_finish-time_start, min_loss_train, min_loss)\n",
    "\n",
    "    def predict(self, X, binar=True):\n",
    "        # Required type(X) equal to np.ndarray with dim = 2\n",
    "        if not type(X) == np.ndarray:\n",
    "            print 'Wrong input type in function \"SVM.predict\", line:', cline()\n",
    "            sys.exit()\n",
    "        if not X.ndim == 2:\n",
    "            print 'Wrong input ndarray size in function \"SVM.predict\", line:', cline()\n",
    "            sys.exit()\n",
    "\n",
    "        if binar:#二分类\n",
    "            return np.sign(np.dot(X, self.wb[:-1]) + self.wb[-1])\n",
    "        else:\n",
    "            return np.dot(X, self.wb[:-1]) + self.wb[-1]\n",
    "\n",
    "    def get_accuracy(self, X, y_true):\n",
    "\n",
    "        if not (X.shape[0] == y_true.shape[0]):\n",
    "            print 'Input arrays shapes mismatching in SVM.get_accuracy, line', cline()\n",
    "\n",
    "        y = self.predict(X)\n",
    "\n",
    "        return np.sum(y == y_true) / float(y.shape[0])\n",
    "\n",
    "    def export(self):\n",
    "        pass\n",
    "\n",
    "def cline():\n",
    "    \"\"\"Returns the current line number in our program.\"\"\"\n",
    "    return inspect.currentframe().f_back.f_lineno\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(\"/home/qian/iNet/PycharmProjects/ML-lab2/a9a\",query_id=True)\n",
    "x_train=np.array(data[0].todense())\n",
    "y_train=np.array(data[1])\n",
    "datatest=load_svmlight_file(\"/home/qian/iNet/PycharmProjects/ML-lab2/a9a.t\",query_id=True,n_features=123)\n",
    "x_test=np.array(datatest[0].todense())\n",
    "y_test=np.array(datatest[1])\n",
    "\n",
    "for opt_algo in opt_algo_set:\n",
    "    clf = SVM(earlystop=20)\n",
    "    clf.fit(x_train, y_train, X_test=x_test, y_test=y_test, opt_algo=opt_algo, n_epoch=200, batch_size=100)\n",
    "    acc_train = clf.get_accuracy(x_train, y_train)\n",
    "    print >> sys.stderr, 'Training accuracy: %lf%%' % (100.0 * acc_train),'of',opt_algo\n",
    "    acc_test = clf.get_accuracy(x_test, y_test)\n",
    "    print >> sys.stderr, 'Test accuracy: %lf%%' % (100.0 * acc_test),'of',opt_algo\n",
    "    testloss = np.array(clf.train_loss_history).reshape(-1, 2)[:, 1].tolist()\n",
    "    plt_dict[opt_algo] = list()\n",
    "    plt_dict[opt_algo].extend(testloss)\n",
    "\n",
    "plt.subplot(111)\n",
    "plt.title('Performance of different Gradient Descent Optimization')\n",
    "plt.xlabel('# of epoch')\n",
    "plt.ylabel('cost')\n",
    "\n",
    "\n",
    "proxy = list()\n",
    "legend_array = list()\n",
    "for index, (opt_algo, epoch_cost) in enumerate(plt_dict.items()):\n",
    "    selected_color = plt_color_array[index % len(plt_color_array)]\n",
    "    plt.plot(range(len(epoch_cost)), epoch_cost, '-%s' % selected_color[0])\n",
    "    proxy.append(Rectangle((0, 0), 0, 0, facecolor=selected_color))\n",
    "    legend_array.append(opt_algo)\n",
    "plt.legend(proxy, legend_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}